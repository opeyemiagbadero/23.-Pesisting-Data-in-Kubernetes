# Pesisting Data in Kubernetes #

Containers are stateless by design, which means that data does not persist in the containers. Even when you run the containers in kubernetes pods, they still remain stateless unless you ensure that your configuration supports statefulness.

To achieve statefuleness in kubernetes, you must understand how volumes, persistent volumes, and persistent volume claims work.

Note: I used Kops for setting up the Kubernetes cluster.

To deploy the Nginx application, I created a manifest file specifying the desired configuration and applied it to the Kubernetes cluster. 

```
sudo cat <<EOF | sudo tee ./nginx-pod.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
EOF
```
Verify that the pod is running by running the command `kubectl apply -f nginx-pod.yaml`

![1aBefore you create a volume, lets run the nginx deployment into kubernetes without a volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/315ddf62-6cbe-4412-b341-533c4b001073)


Exec into the pod and navigate to the nginx configuration file /etc/nginx/conf.d

![1c  Exec into the pod and navigate to the nginx configuration file](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/613bf163-1299-4507-8798-4fc6f05d5272)

Open the config files to see the default configuration.

![1d  Open the config files to see the default configuration](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/c6f943dd-4e2a-42a3-a5f1-a5c6afc38a74)

With the pod running without a volume, I created a volume from the AWS console. In the AWS console, head over to the EC2 section and scroll down to the Elastic Block Storage menu. Click on Volumes. At the top right, click on Create Volume 

![2c create volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/8db15369-28ac-4c24-b498-eb36fec1505a)


Part of the requirements is to ensure that the volume exists in the same region and availability zone as the EC2 instance running the pod. Hence, we need to find out which node is running the pod `kubectl get po nginx-deployment-6449ddf89d-r2rn5 -o wide` and have a look at the ouput.

The node column shows the node the pode is running on

![2a output of the command to confirm the node running the pod](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/bb5420ad-2780-4705-9707-1f2ad7e62c00)

Run the command  `kubectl describe node i-0f9d0ga55704e38f5 ` to give information about the Availability zone in which the the node is running. The information is written in the labels section of the describe command 

![2b the information is written in the labels section of the describe command](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/fa63b84e-4d50-4db9-a59d-b3596dc33eef)


In the case above, we know the AZ for the node is in us-east-1a hence, the volume must be created in the same AZ. 

The create volume section should be like this: 

![2c create volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/7bdf00db-d34c-4887-b108-a2b63fcf9f75)


Update the deployment configuration with the volume spec and mount

![6a update the deployment configuration with the volume spec](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/0a75ab3e-12fa-4874-a648-882be4987027)


Apply the new configuration and check the pod running the command `kubectl apply -f 






