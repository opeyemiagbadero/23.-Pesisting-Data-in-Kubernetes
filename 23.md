# Pesisting Data in Kubernetes #

# Step 1: Introduction #
Containers are stateless by design, which means that data does not persist in the containers. Even when you run the containers in kubernetes pods, they still remain stateless unless you ensure that your configuration supports statefulness.

To achieve statefuleness in kubernetes, you must understand how volumes, persistent volumes, and persistent volume claims work.

Note: I used Kops for setting up the Kubernetes cluster.

To deploy the Nginx application, I created a manifest file specifying the desired configuration and applied it to the Kubernetes cluster. 

```
sudo cat <<EOF | sudo tee ./nginx-pod.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
EOF
```
Verify that the pod is running by running the command `kubectl apply -f nginx-pod.yaml`

![1aBefore you create a volume, lets run the nginx deployment into kubernetes without a volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/315ddf62-6cbe-4412-b341-533c4b001073)


Exec into the pod and navigate to the nginx configuration file /etc/nginx/conf.d

![1c  Exec into the pod and navigate to the nginx configuration file](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/613bf163-1299-4507-8798-4fc6f05d5272)

Open the config files to see the default configuration.

![1d  Open the config files to see the default configuration](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/c6f943dd-4e2a-42a3-a5f1-a5c6afc38a74)

With the pod running without a volume, I created a volume from the AWS console. In the AWS console, head over to the EC2 section and scroll down to the Elastic Block Storage menu. Click on Volumes. At the top right, click on Create Volume 

![2c create volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/8db15369-28ac-4c24-b498-eb36fec1505a)


Part of the requirements is to ensure that the volume exists in the same region and availability zone as the EC2 instance running the pod. Hence, we need to find out which node is running the pod `kubectl get po nginx-deployment-6449ddf89d-r2rn5 -o wide` and have a look at the ouput.

The node column shows the node the pode is running on

![2a output of the command to confirm the node running the pod](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/bb5420ad-2780-4705-9707-1f2ad7e62c00)

Run the command  `kubectl describe node i-0f9d0ga55704e38f5 ` to give information about the Availability zone in which the the node is running. The information is written in the labels section of the describe command 

![2b the information is written in the labels section of the describe command](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/fa63b84e-4d50-4db9-a59d-b3596dc33eef)


In the case above, we know the AZ for the node is in us-east-1a hence, the volume must be created in the same AZ. 

The create volume section should be like this: 

![2c create volume](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/7bdf00db-d34c-4887-b108-a2b63fcf9f75)


Update the deployment configuration with the volume spec

![6a update the deployment configuration with the volume spec](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/0a75ab3e-12fa-4874-a648-882be4987027)


Apply the new configuration and check the pod running the command `kubectl apply -f nginx-pod.yaml`

![6b Apply the new config and check the pod to confirm the updated one is up and running](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/ab2d88f7-03c3-4573-bc40-d5302f080f05)

![6c  update the mount path for the  nginx-pod yaml](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/524850cf-bce1-4d6d-b525-2f728e80246e)

NOtice the newly added session

```
volumeMounts:
        - name: nginx-volume
          mountPath: /usr/share/nginx/
```

The value provided to name in volumeMounts must be the same value used in the volumes section. It basically means mount the volume with the name provided, to the provided mountpath

If you port forward the service and try to reach the endpoint, you might get a 403 error. This is because mounting a volume on a filesystem that already contains data will automatically erase all the existing data. This strategy for statefulness is preferred if the mounted volume already contains the data which you want to be made available to the container

# Step 2: Managing VOumes Dynamically with PV and PVCs #

It is still a manual process to create a volume, manually ensure that the volume created is in the same Avaioability zone in which the pod is running, and then update the manifest file to use the volume ID. All of these is against DevOps principles because it will mean having a lot of road blocks to getting a simple thing done. The more elegant way to achieve this is through Persistent Volume and Persistent Volume claims. In kubernetes, there are many elegant ways of persisting data. Each of which is used to satisfy different use cases. Lets take a look at the different options available.

- Persistent Volume (PV) and Persistent Volume Claim (PVC)
- configMap

PVs are volume plugins that hae a lifecycle completely independent og any individual pod that uses the PV. It is a piece of storage in the cluster that is either provisioned by an administrator through  a manifest tile or can be dynamically created if a storage class has been configured.


If your infrastructure relies on a storage system such as NFS, iSCSI or a cloud provider-specific storage system such as EBS on AWS, then you can dynamically create a PV which will create a volume that a Pod can then use. This means that there must be a storageClass resource in the cluster before a PV can be provisioned.


By default, in Kops, there is a default storageClass configured as part of KOps installation. Verify that there is a storageCLass in the cluster by running the command `kubectl get storageclass`

the ouput should look similar to this 

```
  kubectl get storageclass
  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  18d
```

If there is no storage class in your cluster, below manifest is an example of how one would be created

```
 kind: StorageClass
  apiVersion: storage.k8s.io/v1
  metadata:
    name: gp2
    annotations:
      storageclass.kubernetes.io/is-default-class: "true"
  provisioner: kubernetes.io/aws-ebs
  parameters:
    type: gp2
    fsType: ext4
```


A PersistentVolumeClaim (PVC) on the other hand is a request for storage. Just as Pods consume node resources, PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany).

Now lets create some persistence for our nginx deployment.

Create a manifest file for a PVC, and based on the gp2 storageClass a PV will be dynamically created.

```
 apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: nginx-volume-claim
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
      storageClassName: gp2
```

![6d  Create a manifest tile for a PVC, and based on the gp2 storageClass a PV will be dynamically created](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/f10c01f6-23bf-432c-a4f2-617850105015)

Apply the manifest file and you will get an output as found below

![7a  create a pvc file](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/315119f3-519c-410d-8be7-939c098febe8)

Checking the setup, run the command `$kubectl get pvc` and a kubectl describe pvc nginx-volume-clain

![7b kubectl describe](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/0a3d8c73-32c9-4ef8-b69b-730661997993)

![7c 
Now lets check the dynamically created PV](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/55dfb8e3-c79b-42e8-8458-487e322ace72)


Create a yaml file nginx-deployment-with-pvc

![7bb  create a yaml file nginx-deployment-with-pvc file](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/ef402cfc-aa0e-4693-b75d-dbf4b884a170)

Copy the PV name and serach for it uder the volumes section in the AWS console, and you will notice that the volume has been dynamically created there.

![7d 
You can copy the PV Name and search in the AWS console  You will notice that the volum has been dynamically created there](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/f93dc211-5b17-4cfe-969e-ed071c489d1d)


With the new deployment manifest, the `/tmp/dare` directory will be persisted, and any data written in there will be sotred permanetly on the volume, which can be used by another Pod if the current one gets replaced.


# Step 3: Configmap #

Using configMaps for persistence is not something you would consider for data storage. Rather it is a way to manage configuration files and ensure they are not lost as a result of Pod replacement. To demonstrate this, we will use the HTML file that came with Nginx. This file can be found in `/usr/share/nginx/html/index.html` directory.

Remove the volumeMounts and PVC sections of the nginxanifest and use kubectl to apply the configuration

![10a Remove the volumeMounts and PVC sections of the manifest and use kubectl to apply the configuration](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/49cb3bb6-120b-4e2a-98c0-676c19dff2dd)

Port forward the service and ensure that you are able to see the "Welcome to nginx" page

![10b port forward the service and ensure that you are able to see the  Welcome to nginx  page

exec into the running container and keep a copy of the index html file somewhere ](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/936e1a5d-9f56-433f-ab36-f5f331d9b547)

Exec into the running container and keep a copy of the index.html file somewhere.
kubectl exec -it configmap.pod bash

![10c](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/68256dbd-7d71-413d-9614-00ba630e738f)

Copy the output and save the file on your local pc because we will need it to create a configmap.


![10d output of   cat  usr share nginx html index html 
](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/1c1bf14b-f209-440d-b328-59090e7d1850)

List the available configmaps. You can either use kubectl get configmap or kubectl get cm

![10e List the available configmaps  You can either use kubectl get configmap or kubectl get cm](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/a174e156-3d73-40f7-bc22-65c0aa4fa38c)


Update the configmap file using the command `kubectl edit cm website-index-file `It will open up a vim editor, or whatever default editor your system is configured to use. Update the content as you like. "Only the html data section", then save the file.

![11   Edit the configmap website-index-file edited to reflect darey io and refresh the localhost:8080 on the browser](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/47ae85d8-b023-409c-956b-d859d4aa2ad7)

![12  ls -ltr  usr share nginx html](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/17d0d234-ec8d-4734-8fbf-f44ef294dcd3)

Without restarting the pod, your site should be loaded automatically.

![final page](https://github.com/opeyemiagbadero/23.-Pesisting-Data-in-Kubernetes/assets/79456052/0360aae6-398e-4704-be15-ec96b7832a23)











